{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras version of VGG\n",
    "\n",
    "We'll train VGG with the CIFAR-10 dataset.\n",
    "\n",
    "The Visual Geometry Group (VGG) at the University of Oxford created this model for the 2014 ImageNet competition: [VGG](https://arxiv.org/pdf/1409.1556.pdf).  They tested several versions of the topology (different depths). Typically, the 16 layer (VGG-16) and 19-layer (VGG-19) toplogies are used.\n",
    "\n",
    "Keypoints:\n",
    "+ VGG uses all 3x3 filters\n",
    "+ Receptive field explanation\n",
    "+ 1x1 convolutions - Network in Network\n",
    "+ Data augmentation with random cropping, flipping, color shifting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tonyreina/keras_tutorials/blob/master/lesson_4_VGG.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.layers   import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers   import Input, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.models   import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common terms\n",
    "\n",
    "+ SGD : Stochastic gradient descent. The usual way to train a neural network. The \"weights\" or \"parameters\" of the network are updated bit by bit in order to minimize some global function (\"cost\" or \"loss\")\n",
    "+ \"Cost\" or \" Loss\" - A function we wish the network to minimize. This is typically some distance measure of how far the network's prediction is from the actual value (i.e. the error).\n",
    "+ Epoch = A single pass through the entire training set. SGD involves mulitple passes through the training dataset. \n",
    "+ Batch = How many samples of the training dataset are used to create an update to the weights of the network during SGD. If the batch is 1, then the weights are updated after every forward pass (truly stochastic descent). If the batch is the size of the dataset then the weights are updated based on the sum of the gradients for the entire training set (non-stochastic or just gradient descent). We usually use batch or mini-batch gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size  = 128\n",
    "num_classes = 10\n",
    "epochs      = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10\n",
    "\n",
    "[CIFAR](https://www.cs.toronto.edu/~kriz/cifar.html) is a subset of the \"tiny images\" dataset that includes 80 million images.\n",
    "\n",
    "The CIFAR-10 dataset consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images. \n",
    "\n",
    "The 10 classes are:\n",
    "+ airplane\n",
    "+ automobile\n",
    "+ bird\n",
    "+ cat\n",
    "+ deer\n",
    "+ dog\n",
    "+ frog\n",
    "+ horse\n",
    "+ ship\n",
    "+ truck\n",
    "\n",
    "Tensor size = NHWC = Batch size x 32 x 32 x 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols, n_channels = 32, 32, 3\n",
    "input_shape = (img_rows, img_cols, n_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Scale images between 0 and 1\n",
    "x_train_norm = x_train / 255.\n",
    "x_test_norm  = x_test  / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding\n",
    "\n",
    "For multi-class problems we always one-hot encode the output variable. There are 10 classes (numbers 0-9). The label for 7 would be 0000001000. The label for 0 would be 1000000000. The label for 3 would be 0010000000. This allows us to use the cost function of [multi-class entropy](https://en.wikipedia.org/wiki/Cross_entropy) which will maximize the margin between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG-16\n",
    "\n",
    "![VGG diagram](https://i.stack.imgur.com/3R0Kd.png)\n",
    "\n",
    "Above is VGG-16. In the original specification, the last 3 convolutional blocks use a 1x1 convolution as the last operation. Again, since we only have a 32x32 input image, we'll won't do as many layers (otherwise we would quickly run out of pixels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(input_shape, name='input_images')\n",
    "\n",
    "# Block 1\n",
    "conv1 = Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same')(inputs)\n",
    "conv2 = Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same')(conv1)\n",
    "\n",
    "max2 = MaxPooling2D(pool_size=(2,2))(conv2)\n",
    "\n",
    "# Block 2\n",
    "conv3 = Conv2D(filters=96, kernel_size=(3,3), activation='relu', padding='same')(max2)\n",
    "conv4 = Conv2D(filters=96, kernel_size=(3,3), activation='relu', padding='same')(conv3)\n",
    "\n",
    "max3 = MaxPooling2D(pool_size=(2,2))(conv4)\n",
    "\n",
    "# Block 3\n",
    "conv5 = Conv2D(filters=192, kernel_size=(3,3), activation='relu', padding='same')(max3)\n",
    "conv6 = Conv2D(filters=192, kernel_size=(3,3), activation='relu', padding='same')(conv5)\n",
    "conv7 = Conv2D(filters=192, kernel_size=(1,1), activation='relu', padding='same')(conv6)\n",
    "\n",
    "max4 = MaxPooling2D(pool_size=(2,2))(conv7)\n",
    "\n",
    "# Block 4\n",
    "conv8 = Conv2D(filters=224, kernel_size=(3,3), activation='relu', padding='same')(max4)\n",
    "conv9 = Conv2D(filters=224, kernel_size=(3,3), activation='relu', padding='same')(conv8)\n",
    "conv10 = Conv2D(filters=224, kernel_size=(1,1), activation='relu', padding='same')(conv9)\n",
    "\n",
    "layer11 = Flatten()(conv10)\n",
    "\n",
    "layer12 = Dense(1024, activation='relu')(layer11)\n",
    "\n",
    "layer13 = Dropout(0.5)(layer12)\n",
    "\n",
    "layer14 = Dense(1024, activation='relu')(layer13)\n",
    "\n",
    "layer15 = Dense(num_classes, activation='softmax')(layer14)\n",
    "\n",
    "model = Model(inputs=[inputs], outputs=[layer15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard\n",
    "\n",
    "TensorBoard is an essential tool to monitor our model and the training.  Keras/TF will write a log after every epoch of the model and the current training metrics. All you need to do is type at the command line:\n",
    "\n",
    "tensorboard --logdir='./logs'\n",
    "\n",
    "And then open the browser to http://localhost:6006\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_log = keras.callbacks.TensorBoard(log_dir='./logs', # This is where the log files will go\n",
    "                                     histogram_freq=10, \n",
    "                                     write_graph=True, \n",
    "                                     write_images=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
    "              metrics=['accuracy', keras.metrics.AUC(), keras.metrics.Precision(), keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation\n",
    "\n",
    "VGG used data augmentation to \"add\" more training data and make the model more invariant to rotation and position. Most frameworks have online data augmentation built in. In Keras the [ImageDataGenerator](https://keras.io/preprocessing/image/) creates an iterator that randomly applies flips, rotations, crops, and scalings to the images when each batch is loaded into SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fits the model on batches with real-time data augmentation:\n",
    "history = model.fit(datagen.flow(x_train_norm, y_train, batch_size=batch_size),\n",
    "                    epochs=epochs, \n",
    "                    validation_data=(x_test_norm, y_test), callbacks=[tb_log])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test_norm, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print('Test AUC:', score[2])\n",
    "print('Test Precision:', score[3])\n",
    "print('Test Recall:', score[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss curves\n",
    "\n",
    "It's always a good idea to look at the loss curves. They can tell you if your model is indeed \"learning\" and can point out when it over-fits the training set. TensorBoard is the better way to monitor this, but it can be done also manually with matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,epochs+1),history.history['loss'], '.-',\n",
    "         range(1,epochs+1),history.history['val_loss'], '.-');\n",
    "plt.legend(['training loss', 'testing loss'])\n",
    "plt.title('Loss curve');\n",
    "plt.xlabel('Epoch');\n",
    "plt.ylabel('Loss');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions\n",
    "\n",
    "Now let's use the model to predict the test set images.\n",
    "\n",
    "![cifar10](https://cdn-images-1.medium.com/max/1600/1*6XQqOifwnmplS22zCRRVaw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = model.predict(x_test_norm).argmax(axis=1)\n",
    "print(all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [4, 83, 298, 1045, 3751, 5555, 7112, 8953] # Just print out some random examples from the test set\n",
    "\n",
    "plt.subplots(len(samples)//2, 2, figsize=(10, 16))\n",
    "\n",
    "for i, n in enumerate(samples):\n",
    "\n",
    "    img_norm = np.expand_dims(x_test_norm[n, :, :, :], 0) # Numpy collapses the singleton dimension\n",
    "    img = np.expand_dims(x_test[n, :, :, :], 0) # Numpy collapses the singleton dimension\n",
    "    \n",
    "    plt.subplot(len(samples)//2, 2, i+1)\n",
    "    plt.imshow(img.squeeze());\n",
    "    plt.axis('off')\n",
    "\n",
    "    label = y_test[n].argmax()   \n",
    "    predicted_label = model.predict(img_norm).argmax()   # Predict for just one image\n",
    "\n",
    "    plt.title('Actual = {}, Predicted = {}'.format(labels[label], labels[predicted_label]), fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_vgg_model\")\n",
    "!ls my_vgg_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
